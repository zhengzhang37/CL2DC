import json
import os
import sys
sys.path.append("../")
import logging
from pathlib import Path
import time
import torchvision.models as models
from tqdm import tqdm
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
import torch
from torchvision.models import resnet18
from torch.utils.tensorboard import SummaryWriter
from torcheval import metrics
import copy
# import hydra
from omegaconf import DictConfig, OmegaConf, open_dict
import yaml
import math
from data_provider.chaoyang import *
from data_provider.chestxray import *
from data_provider.micebone import *
from types import SimpleNamespace
from omegaconf import OmegaConf
torch.autograd.set_detect_anomaly(True)
from torch.optim.lr_scheduler import CosineAnnealingLR
import argparse

class Identity(nn.Module):
    def __init__(self):
        super(Identity, self).__init__()

    def forward(self, x):
        return x


class MetaNet(nn.Module):

    def __init__(self, n_classes, pretrained_model, dim_last_layer,
                 n_conv_layers=0,
                 n_linear_layers=0,
                 n_linear_layers_after_concat=1,
                 kernel_sizes_conv=[], width_linear=[],
                 width_linear_after_concat=[],
                 remove_layers=[]):
        super(MetaNet, self).__init__()
        self.pretrained = pretrained_model

        for layer in remove_layers:
            if "." not in layer:
                setattr(self.pretrained, layer, Identity())
            else:
                layer = layer.split(".")
                inner_layer = self.pretrained
                for i in range(len(layer) - 1):
                    inner_layer = getattr(inner_layer, layer[i])
                setattr(inner_layer, layer[-1], Identity())

        self.n_linear_layers = n_linear_layers
        self.conv_layers = []
        self.linear_layers = []
        self.linear_layers_after_concat = []

        for i in range(n_conv_layers):
            if i == 0:
                self.conv_layers.append(nn.Conv2d(dim_last_layer[0],
                                                  dim_last_layer[0],
                                                  kernel_sizes_conv[i]))
                shape_last = ((dim_last_layer[1:] - kernel_sizes_conv[i]) / 2)\
                    + 1
            else:
                self.conv_layers.append(nn.Conv2d(dim_last_layer[0],
                                                  dim_last_layer[0],
                                                  kernel_sizes_conv[i]))
                shape_last = ((shape_last - kernel_sizes_conv[i]) / 2) + 1

        if n_conv_layers == 0:
            shape_last = dim_last_layer[1:]

        self.linear_first_size = shape_last[0] * shape_last[1] * \
            dim_last_layer[0]

        for i in range(n_linear_layers):
            if i == 0:
                self.linear_layers.append(nn.Linear(self.linear_first_size,
                                                    width_linear[i]))
            else:
                self.linear_layers.append(nn.Linear(width_linear[i-1],
                                                    width_linear[i]))

        if n_linear_layers == 0:
            width_linear = [shape_last[0] * shape_last[1] * dim_last_layer[0]]
            self.linear_layers = [Identity()]

        for i in range(n_linear_layers_after_concat):
            if i == 0 and i != n_linear_layers_after_concat - 1:
                self.linear_layers_after_concat.append(
                    nn.Sequential(
                        nn.Linear(n_classes + width_linear[-1],
                                  width_linear_after_concat[i]),
                        nn.ReLU()
                    )
                )
            elif i != n_linear_layers_after_concat - 1:
                self.linear_layers_after_concat.append(
                    nn.Sequential(
                        nn.Linear(width_linear_after_concat[i-1],
                                  width_linear_after_concat[i]),
                        nn.ReLU()
                    )
                )
            elif i != 0:
                self.linear_layers_after_concat.append(
                        nn.Linear(width_linear_after_concat[i], n_classes)
                )
            else:
                self.linear_layers_after_concat.append(
                    nn.Sequential(
                        nn.Linear(n_classes + width_linear[-1],
                                  n_classes),
                    )
                )

        if n_linear_layers_after_concat == 0:
            self.added_layers = nn.Linear(n_classes + width_linear[-1],
                                          n_classes)
        else:
            self.added_layers = nn.Sequential(nn.Linear(256 + n_classes,
                                                        100),
                                              nn.ReLU(),
                                              nn.Linear(100, n_classes))

    def forward(self, x, m):
        x = self.pretrained(x)
        if (len(x.shape) == 4):
            x = F.avg_pool2d(x, 8)
        for layer in self.conv_layers:
            x = layer(x)
            x = F.relu(x)
        x = x.view(-1, self.linear_first_size)
        for layer in self.linear_layers:
            x = layer(x)
            x = F.relu(x)
        if type(m) == list:
            one_hot_m = torch.zeros(len(m), 10)
            one_hot_m[torch.arange(len(m)), torch.tensor(m).long()] = 1
            m = one_hot_m.to(device=DEVICE)
        x = torch.cat((x, m), dim=1)
        for layer in self.linear_layers_after_concat:
            x = layer(x)
        return x

    def to(self, dev):
        self.pretrained.to(dev)
        for layer in self.conv_layers:
            layer.to(dev)
        for layer in self.linear_layers:
            layer.to(dev)
        for layer in self.linear_layers_after_concat:
            layer.to(dev)
        self.added_layers.to(dev)
        return self

    def parameters(self):
        list = [*self.pretrained.parameters()]
        if len(self.conv_layers) != 0:
            for layer in self.conv_layers:
                list += layer.parameters()
        if self.n_linear_layers != 0:
            for layer in self.linear_layers:
                list += layer.parameters()
        if len(self.linear_layers_after_concat) != 0:
            for layer in self.linear_layers_after_concat:
                list += layer.parameters()
        if len(self.added_layers) != 0:
            list += self.added_layers.parameters()
        return list

    def weight_init(self):
        for param in self.parameters():
            if param.requires_grad:
                param.data.uniform_(-1e-5, 1e-5)


def evaluate(
    dataset,
    gating: torch.nn.Module,
    cfg: DictConfig
) -> tuple[torch.Tensor, torch.Tensor, list[torch.Tensor]]:
    """
    Args:
        dataset:
        gating: the gating model
        clf: classifier
        cfg: configuration object

    Returns:

    """


    coverage = metrics.Mean(device=DEVICE)
    accuracy_accum = metrics.MulticlassAccuracy(
        num_classes=cfg.dataset.num_classes,
        device=DEVICE
    )
    selection_accuracy = [metrics.MulticlassAccuracy(num_classes=cfg.dataset.num_classes, device=DEVICE) 
                        for _ in range(cfg.dataset.num_users + 1)]
    # evaluate
    gating.eval()

    for batch_idx, (inputs, labels, humans) in enumerate(dataset):
        # load data
        x, y, t = inputs.to(device=DEVICE), labels.to(device=DEVICE), humans.to(device=DEVICE)

        # gating
        logits_p_z_x = gating.forward(x=x)  # (batch, num_classes + 1)
        selected_expert_ids = torch.argmax(a=logits_p_z_x, dim=-1)  # (batch_size,)
        selected_expert_ids[selected_expert_ids != cfg.dataset.num_classes] = 0
        selected_expert_ids[selected_expert_ids == cfg.dataset.num_classes] = 1
        
        # coverage
        coverage_flag = (selected_expert_ids != 1)
        coverage.update(coverage_flag)

        # classifier
        logits_clf = F.softmax(logits_p_z_x[:,:-1], -1)  # (batch, num_classes)
        human_and_model_predictions = torch.concatenate(
            tensors=(
                logits_clf[:, None, :],
                F.one_hot(input=t, num_classes=cfg.dataset.num_classes)[:, None, :]
            ),
            dim=1
        )

        # defer
        queried_predictions = human_and_model_predictions[torch.arange(len(x)), selected_expert_ids, :]
        accuracy_accum.update(
            input=torch.argmax(input=queried_predictions, dim=-1),
            target=y
        )

        for expert_id in range(cfg.dataset.num_users + 1):
            expert_mask = selected_expert_ids == expert_id
            if expert_mask.sum() > 0:
                expert_preds = queried_predictions[expert_mask]
                expert_targets = y[expert_mask]
                selection_accuracy[expert_id].update(expert_preds, expert_targets)
    
    return accuracy_accum.compute(), coverage.compute(), [selection_accuracy[i].compute() for i in range(len(selection_accuracy))]

def create_clf(args):
    if args.dataset.name == 'chaoyang':
        model = models.resnet34(pretrained=True)
        model.fc = nn.Linear(512, 4)
        model.load_state_dict(torch.load('../pretrained_models/chaoyang/best_ckpt.pth'))
    elif args.dataset.name == 'chestxray':
        model = models.resnet18(pretrained=True)
        model.fc = nn.Linear(512, 2)
        model.load_state_dict(torch.load('../pretrained_models/chestxray/best_ckpt.pth'))
    elif args.dataset.name == 'micebone':
        model = models.resnet18(pretrained=True)
        model.fc = nn.Linear(512, 3)
        model.load_state_dict(torch.load('../pretrained_models/micebone/best_ckpt.pth'))
    for param in model.parameters():
        param.requires_grad = False
    model_classifier = copy.deepcopy(model)
    model_classifier.fc = nn.Linear(512, cfg.dataset.num_classes+1)
    model_human = copy.deepcopy(model)
    model_meta = MetaNet(cfg.dataset.num_classes, copy.deepcopy(model),
                                 [1, 50, 1],
                                 remove_layers=["fc"])
    return model_classifier.to(device=DEVICE), model_human.to(device=DEVICE), model_meta.to(device=DEVICE)

def LossBCEH(outputs, y, m):
    num_classes = outputs.size()[1]
    y_oh = F.one_hot(y, num_classes=num_classes).float()
    y_oh[:, -1] = (m == y).float()
    return F.binary_cross_entropy_with_logits(outputs, y_oh,
                                                reduction='none').sum(dim=1)
def LossBCE(outputs, y):
    num_classes = outputs.size()[1]
    y_oh = F.one_hot(y, num_classes=num_classes).float()
    return F.binary_cross_entropy_with_logits(outputs, y_oh,
                                                reduction='none').sum(dim=1)

def surrogate_loss_bce(out_class, outputs_sim, outputs_meta, m,
                           data_y):
    """
    outputs: network outputs
    m: cost of deferring to expert cost of classifier predicting
        hum_preds == target
    labels: target
    """
    return (LossBCEH(out_class, data_y, m)
            + LossBCE(outputs_sim, m)
            + LossBCE(outputs_meta, data_y)).mean()

def create_model(args):
    if args.dataset.name == 'chaoyang':
        model = models.resnet34(pretrained=True)
        model.fc = nn.Linear(512, 4)
        model.load_state_dict(torch.load('../pretrained_models/chaoyang/best_ckpt.pth'))
    elif args.dataset.name == 'chestxray':
        model = models.resnet18(pretrained=True)
        model.fc = nn.Linear(512, 2)
        model.load_state_dict(torch.load('../pretrained_models/chestxray/best_ckpt.pth'))
    elif args.dataset.name == 'micebone':
        model = models.resnet18(pretrained=True)
        model.fc = nn.Linear(512, 3)
        model.load_state_dict(torch.load('../pretrained_models/micebone/best_ckpt.pth'))
    for param in model.parameters():
        param.requires_grad = False
    model.fc = nn.Linear(512, args.dataset.num_users+args.dataset.num_classes)
    return model.to(device=DEVICE)

def main(cfg, args):
    """
    """
    cfg = OmegaConf.create(obj=cfg)
    OmegaConf.set_struct(conf=cfg, value=True)

    if cfg.dataset.name == 'chaoyang':
        loader = chaoyang_dataloader(batch_size=cfg.training.batch_size, num_workers=cfg.training.num_workers)
    elif cfg.dataset.name == 'chestxray':
        loader = chestxray_dataloader(batch_size=cfg.training.batch_size, num_workers=cfg.training.num_workers, args=cfg)
    elif cfg.dataset.name == 'micebone':
        loader = micebone_dataloader(batch_size=cfg.training.batch_size, num_workers=cfg.training.num_workers)
    train_loader = loader.run(mode='train')
    test_loader = loader.run(mode='test')
    
    with open_dict(config=cfg):
        cfg.dataset.length = cfg.dataset.num_users+1

    model_classifier, model_sim, model_meta = create_model(cfg)

    optimiser = torch.optim.SGD(
        [
            {'params': model_classifier.parameters()},
            {'params': model_sim.parameters()},
            {'params': model_meta.parameters()}
        ],
        lr=args.lr,
        momentum=0.9, 
        nesterov=True,
        weight_decay=5e-4
    )
    scheduler = CosineAnnealingLR(optimiser, len(train_loader) * 300, eta_min=0)
    # create a directory for storage (if not existed)
    logdir = os.path.join(cfg.experiment.logdir, cfg.dataset.name, 'coverage_'+str(args.coverage))
    modeldir = os.path.join(cfg.experiment.modeldir, cfg.dataset.name, 'coverage_'+str(args.coverage))
    if not os.path.exists(path=logdir):
        Path(logdir).mkdir(parents=True, exist_ok=True)
    if not os.path.exists(path=modeldir):
        Path(modeldir).mkdir(parents=True, exist_ok=True)

    try:
        # tensorboard writer
        writer = SummaryWriter(
            log_dir=logdir,
            filename_suffix='learning_to_defer'
        )
        model_classifier.train()
        model_meta.train()
        model_sim.train()
        for epoch_id in tqdm(
            iterable=range(0, cfg.training.num_epochs, 1),
            desc='progress',
            ncols=80,
            leave=True,
            position=1,
            colour='green',
            disable=not cfg.training.progress_bar
        ):
            loss_accum = metrics.Mean(device=DEVICE)
            for batch_idx, (inputs, labels, expert) in enumerate(train_loader):
                inputs = inputs.to(device=DEVICE)
                labels = labels.to(device=DEVICE)
                expert = expert.to(device=DEVICE)
                one_hot_m = F.one_hot(input=expert, num_classes=cfg.dataset.num_classes)
                outputs_classifier = model_classifier(inputs)
                outputs_meta = model_meta(inputs, one_hot_m)
                outputs_sim = model_sim(inputs)

                loss = surrogate_loss_bce(outputs_classifier, outputs_sim,
                                                outputs_meta, expert, labels)
                optimiser.zero_grad()
                exp_coverage = F.softmax(input=output, dim=-1)[:,:-1].sum() / len(output) 
                hyper_params = epoch_id + 1
                loss = loss + hyper_params * args.h * (args.coverage - exp_coverage) ** 2
                loss.backward()
                optimiser.step()
                scheduler.step()
                loss_accum.update(loss)
            # evaluation
            accuracy, coverage, selection_accuracy = evaluate(dataset=test_loader, gating=model, cfg=cfg)
            # torch.save(model.state_dict(), os.path.join(modeldir, 'model.pth'))
            table_header = "| Exp Coverage | Real Coverage | Accuracy |\n|------------|---------------|------------|\n"
            table_rows = [f"| {args.coverage} | {coverage} | {accuracy} |"]
            table_content = table_header + "\n".join(table_rows)

            # write to tensorboard

            writer.add_text("Experiment Results", table_content)
            
            writer.add_scalar(
                tag='loss',
                scalar_value=loss_accum.compute(),
                global_step=epoch_id + 1
            )

            writer.add_scalars(
                main_tag='accuracy',
                tag_scalar_dict={
                    'system': accuracy,
                    **{'selection' + f'{i}': selection_accuracy[i] for i in range(len(selection_accuracy))}
                },
                global_step=epoch_id + 1
            )
            writer.add_scalar(
                tag='coverage',
                scalar_value=coverage,
                global_step=epoch_id + 1
            )
    finally:
        writer.close()

if __name__ == '__main__':
    # DEVICE = torch.device(device='cuda:1') if torch.cuda.is_available() else torch.device(device='cpu')


    parser = argparse.ArgumentParser(description='PyTorch Chaoyang Training')
    parser.add_argument('--coverage', type=float, default=0.1)
    parser.add_argument('--lr', type=float, default=0.01)
    parser.add_argument('--h', type=int, default=2)
    args = parser.parse_args()
    cfg = OmegaConf.load('../config/daf_chaoyang_conf.yaml')
    DEVICE = torch.device(device='cuda:{:d}'.format(cfg.training.device)) if torch.cuda.is_available() else torch.device(device='cpu')
    coverage_list = [0.2, 0.4, 0.6, 0.8]
    for coverage in coverage_list:
        args.coverage = coverage
        main(cfg, args)



